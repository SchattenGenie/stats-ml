{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Sum}{\\sum\\limits}$\n",
    "$\\newcommand{\\Int}{\\int\\limits}$\n",
    "$\\newcommand{\\Intf}{\\int\\limits_{-\\infty}^{+\\infty}}$\n",
    "$\\newcommand{\\Prod}{\\prod\\limits}$\n",
    "$\\newcommand{\\Max}{\\max\\limits}$\n",
    "$\\newcommand{\\Min}{\\min\\limits}$\n",
    "$\\newcommand{\\Lim}{\\lim\\limits}$\n",
    "$\\newcommand{\\Var}{\\mathbb{V}}$\n",
    "$\\newcommand{\\Exp}{\\mathbb{E}}$\n",
    "$\\newcommand{\\argmax}{\\arg\\max}$\n",
    "$\\newcommand{\\Cov}{\\mathrm{Cov}}$\n",
    "$\\newcommand{\\makebold}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\mean}[1]{\\overline{#1}}$\n",
    "$\\newcommand{\\avg}[1]{\\left\\langle #1 \\right\\rangle}$\n",
    "$\\newcommand{\\Prob}{\\mathcal{P}}$\n",
    "$\\newcommand{\\lp}{\\left}$\n",
    "$\\newcommand{\\rp}{\\right}$\n",
    "$\\newcommand{\\eps}{\\varepsilon}$\n",
    "$\\newcommand{\\loss}{\\mathcal{L}}$\n",
    "$\\newcommand{\\Llr}{\\mathcal{L}}$\n",
    "$\\newcommand{\\llr}{\\ell}$\n",
    "\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\ZZ}{\\mathbb{Z}}$\n",
    "$\\newcommand{\\NN}{\\mathbb{N}}$\n",
    "\n",
    "$\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\boldtau}{\\boldsymbol{\\tau}}$\n",
    "$\\newcommand{\\boldX}{\\boldsymbol{X}}$\n",
    "$\\newcommand{\\boldY}{\\boldsymbol{Y}}$\n",
    "$\\newcommand{\\boldZ}{\\boldsymbol{Z}}$\n",
    "$\\newcommand{\\boldtheta}{\\boldsymbol{\\theta}}$\n",
    "$\\newcommand{\\boldtau}{\\boldsymbol{\\tau}}$\n",
    "\n",
    "$\\newcommand{\\boldx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\boldu}{\\boldsymbol{u}}$\n",
    "$\\newcommand{\\boldv}{\\boldsymbol{v}}$\n",
    "$\\newcommand{\\boldy}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\boldz}{\\boldsymbol{z}}$\n",
    "$\\newcommand{\\boldp}{\\boldsymbol{p}}$\n",
    "\n",
    "$\\newcommand{\\Poisson}{\\mathrm{Poisson}}$\n",
    "$\\newcommand{\\Uniform}{\\mathrm{Uniform}}$\n",
    "$\\newcommand{\\Binomial}{\\mathrm{Binomial}}$\n",
    "$\\newcommand{\\Gammap}{\\mathrm{Gamma}}$\n",
    "$\\newcommand{\\Normal}{\\mathcal{N}}$\n",
    "$\\newcommand{\\LogN}{\\mathrm{LogN}}$\n",
    "$\\newcommand{\\Exponential}{\\mathrm{Exp}}$\n",
    "$\\newcommand{\\Erlang}{\\mathrm{Erlang}}$\n",
    "$\\newcommand{\\Cauchy}{C}$\n",
    "\n",
    "$\\newcommand{\\lf}{\\left\\{}$\n",
    "$\\newcommand{\\rf}{\\right\\}}$\n",
    "$\\newcommand{\\lp}{\\left(}$\n",
    "$\\newcommand{\\rp}{\\right)}$\n",
    "$\\newcommand{\\ls}{\\left[}$\n",
    "$\\newcommand{\\rs}{\\right]}$\n",
    "$\\newcommand{\\lv}{\\left|}$\n",
    "$\\newcommand{\\rv}{\\right|}$\n",
    "\n",
    "$\\newcommand{\\Ecdf}[1]{\\hat{F}_n(#1)}$\n",
    "$\\newcommand{\\boot}{\\mathrm{boot}}$\n",
    "$\\newcommand{\\bias}{\\mathrm{bias}}$\n",
    "$\\newcommand{\\se}{\\mathrm{se}}$\n",
    "$\\newcommand{\\MSE}{\\mathrm{MSE}}$\n",
    "$\\newcommand{\\qm}{\\mathrm{qm}}$\n",
    "$\\newcommand{\\as}{\\mathrm{as}}$\n",
    "$\\newcommand{\\trace}{\\mathrm{trace}}$\n",
    "\n",
    "$\\newcommand{\\esttheta}{\\hat{\\theta}}$\n",
    "$\\newcommand{\\estlambda}{\\hat{\\lambda}}$\n",
    "$\\newcommand{\\estmu}{\\hat{\\mu}}$\n",
    "$\\newcommand{\\estsigma}{\\hat{\\sigma}}$\n",
    "$\\newcommand{\\estalpha}{\\hat{\\alpha}}$\n",
    "$\\newcommand{\\estbeta}{\\hat{\\beta}}$\n",
    "$\\newcommand{\\estxi}{\\hat{\\xi}}$\n",
    "$\\newcommand{\\esttau}{\\hat{\\tau}}$\n",
    "$\\newcommand{\\estpsi}{\\hat{\\psi}}$\n",
    "$\\newcommand{\\esta}{\\hat{a}}$\n",
    "$\\newcommand{\\estb}{\\hat{b}}$\n",
    "$\\newcommand{\\estc}{\\hat{c}}$\n",
    "$\\newcommand{\\estd}{\\hat{d}}$\n",
    "$\\newcommand{\\estp}{\\hat{p}}$\n",
    "$\\newcommand{\\estT}{\\hat{T}}$\n",
    "$\\newcommand{\\estR}{\\hat{R}}$\n",
    "$\\newcommand{\\estF}{\\hat{F}}$\n",
    "$\\newcommand{\\estf}{\\hat{f}}$\n",
    "$\\newcommand{\\estC}{\\hat{C}}$\n",
    "$\\newcommand{\\estS}{\\hat{S}}$\n",
    "$\\newcommand{\\estY}{\\hat{Y}}$\n",
    "$\\newcommand{\\esty}{\\hat{y}}$\n",
    "$\\newcommand{\\estVar}{\\hat{\\Var}}$\n",
    "$\\newcommand{\\estExp}{\\hat{\\Exp}}$\n",
    "$\\newcommand{\\estSe}{\\hat{\\se}}$\n",
    "\n",
    "$\\newcommand{\\hata}{\\hat{a}}$\n",
    "$\\newcommand{\\hatb}{\\hat{b}}$\n",
    "$\\newcommand{\\hatc}{\\hat{c}}$\n",
    "$\\newcommand{\\hatd}{\\hat{d}}$\n",
    "$\\newcommand{\\hatf}{\\hat{f}}$\n",
    "$\\newcommand{\\hatg}{\\hat{g}}$\n",
    "$\\newcommand{\\hatk}{\\hat{k}}$\n",
    "$\\newcommand{\\hatp}{\\hat{p}}$\n",
    "$\\newcommand{\\hatr}{\\hat{r}}$\n",
    "$\\newcommand{\\hatt}{\\hat{t}}$\n",
    "$\\newcommand{\\haty}{\\hat{y}}$\n",
    "\n",
    "$\\newcommand{\\hatC}{\\hat{C}}$\n",
    "$\\newcommand{\\hatF}{\\hat{F}}$\n",
    "$\\newcommand{\\hatJ}{\\hat{J}}$\n",
    "$\\newcommand{\\hatK}{\\hat{K}}$\n",
    "$\\newcommand{\\hatY}{\\hat{Y}}$\n",
    "\n",
    "$\\newcommand{\\tiltau}{\\tilde{\\tau}}$\n",
    "$\\newcommand{\\tiltheta}{\\tilde{\\theta}}$\n",
    "$\\newcommand{\\tillambda}{\\tilde{\\lambda}}$\n",
    "$\\newcommand{\\tilsigma}{\\tilde{\\sigma}}$\n",
    "\n",
    "$\\newcommand{\\mlexi}{\\xi_{MLE}}$\n",
    "$\\newcommand{\\mletheta}{\\theta_{MLE}}$\n",
    "$\\newcommand{\\mlelambda}{\\lambda_{MLE}}$\n",
    "$\\newcommand{\\mlesigma}{\\sigma_{MLE}}$\n",
    "\n",
    "$\\newcommand{\\mmxi}{\\xi_{MM}}$\n",
    "$\\newcommand{\\mmtheta}{\\theta_{MM}}$\n",
    "$\\newcommand{\\mmlambda}{\\lambda_{MM}}$\n",
    "$\\newcommand{\\mmsigma}{\\sigma_{MM}}$\n",
    "$\\newcommand{\\mmgamma}{\\gamma_{MM}}$\n",
    "\n",
    "$\\newcommand{\\xs}[1]{\\boldx^{(#1)}}$\n",
    "$\\newcommand{\\ys}[1]{\\boldy^{(#1)}}$\n",
    "$\\newcommand{\\zs}[1]{\\boldz^{(#1)}}$\n",
    "$\\newcommand{\\Xs}[1]{\\boldX^{(#1)}}$\n",
    "$\\newcommand{\\Ys}[1]{\\boldY^{(#1)}}$\n",
    "$\\newcommand{\\Zs}[1]{\\boldZ^{(#1)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import copy\n",
    "\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import combinations, product\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    " \n",
    "# sklearn\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.kernel_ridge import KernelRidge, pairwise_kernels\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.colors import colorConverter\n",
    "%matplotlib inline\n",
    "\n",
    "legendsize = 22\n",
    "xticksize = 18\n",
    "yticksize = xticksize\n",
    "\n",
    "matplotlib.rcParams['legend.markerscale'] = 1.5     # the relative size of legend markers vs. original\n",
    "matplotlib.rcParams['legend.handletextpad'] = 0.5\n",
    "matplotlib.rcParams['legend.labelspacing'] = 0.4    # the vertical space between the legend entries in fraction of fontsize\n",
    "matplotlib.rcParams['legend.borderpad'] = 0.5       # border whitespace in fontsize units\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=xticksize)\n",
    "matplotlib.rc('ytick', labelsize=yticksize)\n",
    "matplotlib.rc('legend', fontsize=legendsize)\n",
    "\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rc('text.latex', unicode=True)\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage[english]{babel}')\n",
    "matplotlib.rc('text.latex', preamble=r'\\usepackage{amsmath}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "# Содержание\n",
    "* [Гистограмная оценка плотности](#nonparam_pdf_hist)\n",
    "* [Ядерная оценка плотности](#nonparam_pdf_kernel)\n",
    "* [Непараметрическая регрессия](#nonparam_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nonparam_pdf_hist'></a>\n",
    "## Гистограмная оценка плотности [[toc]](#toc) [[up]](#nonparam_pdf)\n",
    "* [1. Выбор оптимальной ширины бинов](#hist_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Для выбора оптимальной ширины бина нужно найти минимум следующего функционала(эмпирического риска):\n",
    "\n",
    "\\begin{gather*}\n",
    "J(h) = \\int \\hat{p}^2(x) dx - 2 \\int \\hat{p}(x) p(x) dx\n",
    "\\end{gather*}\n",
    "\n",
    "**Почему эмпирический риск так выглядит? Каков его смысл?**\n",
    "\n",
    "Эквивалентная изначальной формуле, но более удобная для вычисления:\n",
    "\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{2}{(n - 1)h} - \\frac{n+1}{(n-1)h}\\Sum_{i = 1}^m \\hatp_j^2,\\quad \\hatp_j = \\frac{n_j}{n}.\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.histogram`\n",
    "\n",
    "Упраженение на ловкость рук. Написать своё разбиение на бины.\n",
    "\n",
    "Возвращаться должны края бинов. Т.е. такой выход:\n",
    "\n",
    "> [1, 2, 3, 4]\n",
    "\n",
    "означает что первый бин это [1, 2) (включая 1, но исключая 2), а второй [2, 3). Последний(третий бин) -- [3, 4] и он включает 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binedges(center, bandwidth, x_left, x_right):\n",
    "    \"\"\"\n",
    "    Функция для создания массива краёв бинов. \n",
    "    На вход приходит центр, ширина бина, левый край и правый край.\n",
    "    \"\"\"\n",
    "    h = bandwidth\n",
    "    bins = # <YOUR CODE>\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(create_binedges(0.5, 0.2, 0., 1.), [0.0, 0.2, 0.4, 0.6, 0.8, 1.0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_pdf_estimated_risk(bincounts, bandwidth):\n",
    "    \"\"\"\n",
    "    Функция оценки эмпирического \n",
    "    \"\"\"\n",
    "    h = bandwidth\n",
    "    n = np.sum(bincounts)\n",
    "    ps = bincounts / n\n",
    "    J = # <YOUR CODE>\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(histogram_pdf_estimated_risk([1, 2, 3, 4, 5, 6, 7], 0.1), -1.1772486772486772)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hist_pdf_opt_bandwidth'></a>\n",
    "## 1. Выбор оптимальной ширины бинов [[toc]](#toc) [[#up]](up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Создание выборки [[toc]](#toc) [[up]](#hist_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 100\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Функция $\\hatJ(h)$ оценки риска [[toc]](#toc) [[up]](#hist_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.01, 10, 1000)\n",
    "risk_values = []\n",
    "for bandwidth in bandwidth_range:\n",
    "    binedges = create_binedges(0, bandwidth, samples.min(), samples.max())\n",
    "    bincounts, _ = np.histogram(samples, bins=binedges)\n",
    "    risk_values.append(histogram_pdf_estimated_risk(bincounts, bandwidth))\n",
    "risk_values = np.array(risk_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(bandwidth_range, risk_values, color='b', label=r'$\\hat{J}(h)$', zorder=2);\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmin(risk_values)\n",
    "opt_bandwidth = bandwidth_range[index_min]\n",
    "min_risk = risk_values[index_min]\n",
    "print('h = {}, J = {}'.format(opt_bandwidth, min_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 PDF для оптимальной ширины ядра [[toc]](#toc) [[up]](#hist_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binedges = create_binedges(0, opt_bandwidth, samples.min(), samples.max())\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(samples, bins=binedges, color='b', density=True, edgecolor='k', \n",
    "         label='estimated pdf', alpha=0.5, zorder=2);\n",
    "plt.plot(x_values, true_pdf, color='r', label='true pdf', zorder=2);\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nonparam_pdf_kernel'></a>\n",
    "# Ядерная оценка плотности [[toc]](#toc) [[up]](#nonparam_pdf)\n",
    "* [1. Simple 1D Kernel Density Estimation](#kernel_pdf_ex1)\n",
    "* [2. Kernel Density Estimation](#kernel_pdf_digits)\n",
    "* [3. Выбор оптимальной ширины ядра](#kernel_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather*}\n",
    "J(h) = \\int \\hatp^2(x) dx - 2 \\int \\hatp(x) p(x) dx\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{gather*}\n",
    "MISE = \\frac{1}{4} \\sigma_K^4 h^4 \\int (p''(x))^2 dx + \\frac{1}{nh} \\int \\left( K(x) \\right)^2 dx\n",
    "\\end{gather*}\n",
    "\n",
    "**Что оказывает наибольшее влияние на качество оценки ядерной плотностью?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вычислениях пользуемся оценкой эмпирического риска вида:\n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\frac{1}{hn^2}\\Sum_{i = 1}^n\\Sum_{j = 1}^n K^{(2)}\\lp\\frac{x_i - x_j}{h}\\rp + 2 \\frac{K(0)}{nh},\n",
    "\\end{gather*}\n",
    "где\n",
    "\\begin{gather*}\n",
    "K^{(2)}(x) = K^*(x) - 2K(x), \\quad K^*(x) = \\int K(x - y) K(y) dy.\n",
    "\\end{gather*}\n",
    "\n",
    "Подробности о формуле выше: http://www.stat.cmu.edu/~larry/=sml/densityestimation.pdf (27)\n",
    "\n",
    "Далее приводятся примеры [ядерное оценивание плотности из библиотеки sklearn](http://scikit-learn.org/stable/modules/density.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дня нормального распределения:\n",
    "\n",
    "\\begin{gather*}\n",
    "K^*(x)= N(x | 0, \\sqrt{2})\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_estimated_risk(samples, bandwidth):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    -samples: 1D numpy array with samples\n",
    "    -bandwidth: bandwidth parameter of gaussian kernel\n",
    "    \n",
    "    Returns estimated risk value.\n",
    "    \"\"\"\n",
    "    X = samples\n",
    "    h = bandwidth\n",
    "    n = len(X)\n",
    "    Jh = 0\n",
    "\n",
    "    Jh = # <YOUR CODE>\n",
    "    return Jh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(gaussian_kernel_estimated_risk(np.array([1, 2, 3, 4, 5, 6]), 0.1), 0.47015798630067906)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_pdf_ex1'></a>\n",
    "## 1. 1D Kernel Density Estimation [[toc]](#toc) [[up]](#nonparam_pdf_kernel)\n",
    "\n",
    "В этом примере показывается как работает `sklearn.neighbors.KernelDensity` в одномерном случае.\n",
    "\n",
    "Преимуществом применения `sklearn.neighbors.KernelDensity` в том что в нём реализованы эффективные алгоритмы для расчёта плотности, котые используют Ball-Tree или KD-Tree для ускорения расчётов метрик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the progression of histograms to kernels  [[toc]](#toc) [[up]](#kernel_pdf_ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 100\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "alpha = 0.01; gamma = 0.25\n",
    "x_left = np.percentile(samples, 100 * alpha)\n",
    "x_right = np.percentile(samples, 100 * (1 - alpha))\n",
    "x_width = x_right - x_left\n",
    "x_left  -= gamma * x_width\n",
    "x_right += gamma * x_width\n",
    "print(x_left, x_right, x_width)\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "# histogram 1\n",
    "ax[0, 0].hist(samples, bins=binedges, fc='#AAAAFF', density=True, edgecolor='k')\n",
    "ax[0, 0].set_title(\"Histogram\")\n",
    "\n",
    "# histogram 2\n",
    "ax[0, 1].hist(samples, bins=binedges + 0.75, fc='#AAAAFF', density=True, edgecolor='k')\n",
    "ax[0, 1].set_title(\"Histogram, bins shifted\")\n",
    "\n",
    "# tophat KDE\n",
    "kde =  #<YOUR CODE>\n",
    "log_dens = #<YOUR CODE>\n",
    "#<YOUR CODE>\n",
    "ax[1, 0].plot(x_values, np.exp(log_dens), color='k', zorder=2)\n",
    "ax[1, 0].fill(x_values, np.exp(log_dens), fc='#AAAAFF', zorder=2)\n",
    "ax[1, 0].set_title(\"Tophat Kernel Density\")\n",
    "\n",
    "# Gaussian KDE\n",
    "kde =  #<YOUR CODE>\n",
    "log_dens = #<YOUR CODE>\n",
    "\n",
    "ax[1, 1].plot(x_values, np.exp(log_dens), color='k', zorder=2)\n",
    "ax[1, 1].fill(x_values, np.exp(log_dens), fc='#AAAAFF')\n",
    "ax[1, 1].set_title(\"Gaussian Kernel Density\")\n",
    "\n",
    "for n_row, n_col in product(range(2), range(2)):\n",
    "    ax[n_row, n_col].grid(which='both', linestyle='--', alpha=0.5)\n",
    "    # Plotting true PDF on each subplot\n",
    "    ax[n_row, n_col].plot(x_values, true_pdf, color='r', linestyle='--', zorder=2);\n",
    "    \n",
    "for axi in ax.ravel():\n",
    "    axi.plot(samples, np.zeros(len(samples)) - 0.01, '+k')\n",
    "    axi.set_xlim(x_left, x_right)\n",
    "    axi.set_ylim(-0.02, 0.34)\n",
    "\n",
    "for axi in ax[:, 0]:\n",
    "    axi.set_ylabel('Normalized Density')\n",
    "\n",
    "for axi in ax[1, :]:\n",
    "    axi.set_xlabel('$x$')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выведем ядра на одном экране [[toc]](#toc) [[up]](#kernel_pdf_ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = np.linspace(-6, 6, 1000)[:, None]\n",
    "X_src = np.zeros((1, 1))\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(12, 6))\n",
    "#fig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n",
    "\n",
    "def format_func(x, loc):\n",
    "    if x == 0:\n",
    "        return '0'\n",
    "    elif x == 1:\n",
    "        return 'h'\n",
    "    elif x == -1:\n",
    "        return '-h'\n",
    "    else:\n",
    "        return '%ih' % x\n",
    "# Давайте пройдёмся по документации и выпишем все ядра.\n",
    "for i, kernel in enumerate([]): # <YOUR KERNELS>\n",
    "    axi = ax.ravel()[i]\n",
    "    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(x_values) #<YOUR ONE LINE CODE>\n",
    "    axi.plot(x_values[:, 0], np.exp(log_dens), color='k', zorder=2);\n",
    "    axi.fill(x_values[:, 0], np.exp(log_dens), '-k', fc='#AAAAFF');\n",
    "    axi.set_title(kernel)\n",
    "\n",
    "    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    axi.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    axi.set_ylim(0, 1.05)\n",
    "    axi.set_xlim(-2.9, 2.9)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим часть из них [[toc]](#toc) [[up]](#kernel_pdf_ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 100\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.fill(x_values, true_pdf, fc='black', alpha=0.2, label='input distribution')\n",
    "\n",
    "for kernel in ['gaussian', 'tophat', 'epanechnikov']:\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(samples[:,None])\n",
    "    log_dens = kde.score_samples(x_values[:, None])\n",
    "    ax.plot(x_values, np.exp(log_dens), '-', label=\"kernel = ``{0}\\\"\".format(kernel))\n",
    "\n",
    "ax.set_title(\"N={0} points\".format(n_samples))\n",
    "ax.legend(loc='upper left')\n",
    "ax.plot(samples, -0.005 - 0.01 * np.random.random(samples.shape[0]), '+k')\n",
    "\n",
    "ax.set_xlim(-4, 9)\n",
    "ax.set_ylim(-0.02, 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_pdf_digits'></a>  \n",
    "## 2. Kernel Density Estimation [[toc]](#toc) [[up]](#nonparam_pdf_kernel)\n",
    "\n",
    "А сейчас мы сделаем GAN, но... без нейросеток! \n",
    "\n",
    "Будем сэмплировать картинки из KDE ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "print('Initial data shape: data.shape = {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the 64-dimensional data to a lower dimension\n",
    "\n",
    "# we can play with the number of components\n",
    "pca = PCA(n_components=15, whiten=False)\n",
    "data = pca.fit_transform(digits.data)\n",
    "print('After dimensionality reduction: data.shape = {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search cross-validation to optimize the bandwidth\n",
    "params = {'bandwidth': np.logspace(-1, 1, 20)}\n",
    "grid = GridSearchCV(KernelDensity(), params)\n",
    "grid.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
    "\n",
    "# use the best estimator to compute the kernel density estimate\n",
    "kde = grid.best_estimator_\n",
    "\n",
    "# sample 44 new points from the data\n",
    "new_data = kde.sample(44, random_state=0)\n",
    "new_data = pca.inverse_transform(new_data)\n",
    "\n",
    "# turn data into a 4x11 grid\n",
    "new_data = new_data.reshape((4, 11, -1))\n",
    "real_data = digits.data[:44].reshape((4, 11, -1))\n",
    "\n",
    "# plot real digits and resampled digits\n",
    "fig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\n",
    "for j in range(11):\n",
    "    ax[4, j].set_visible(False)\n",
    "    for i in range(4):\n",
    "        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),\n",
    "                             cmap=plt.cm.binary, interpolation='nearest')\n",
    "        im.set_clim(0, 16)\n",
    "        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),\n",
    "                                 cmap=plt.cm.binary, interpolation='nearest')\n",
    "        im.set_clim(0, 16)\n",
    "\n",
    "ax[0, 5].set_title('Selection from the input data')\n",
    "ax[5, 5].set_title('``New\" digits drawn from the kernel density model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kernel_pdf_opt_bandwidth'></a>\n",
    "## 3. Выбор оптимальной ширины ядра [[toc]](#toc) [[up]](#nonparam_pdf_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Создание выборки [[toc]](#toc) [[up]](#kernel_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "n_samples = 100\n",
    "gen1 = scipy.stats.norm(loc=0, scale=1)\n",
    "gen2 = scipy.stats.norm(loc=5, scale=1)\n",
    "p1 = 0.3\n",
    "p2 = 1 - p1\n",
    "samples1 = gen1.rvs(size=int(p1 * n_samples), random_state=seed)       # Samples from the first components\n",
    "samples2 = gen2.rvs(size=int(p2 * n_samples), random_state=seed + 1)   # Samples from the second components\n",
    "samples = np.concatenate([samples1, samples2])                         # All samples\n",
    "\n",
    "x_values = np.linspace(-5, 10, 1000)\n",
    "binedges = np.linspace(-5, 10, 10)\n",
    "true_pdf = p1 * gen1.pdf(x_values) + p2 * gen2.pdf(x_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Функция $\\hatJ(h)$ оценки риска [[toc]](#toc) [[up]](#kernel_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.01, 10, 1000)\n",
    "risk_values = []\n",
    "for bandwidth in bandwidth_range:\n",
    "    risk_values.append(gaussian_kernel_estimated_risk(samples, bandwidth))\n",
    "risk_values = np.array(risk_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(bandwidth_range, risk_values, color='b', label=r'$\\hat{J}(h)$', zorder=2);\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmin(risk_values)\n",
    "opt_bandwidth = bandwidth_range[index_min]\n",
    "min_risk = risk_values[index_min]\n",
    "print('h = {}, J = {}'.format(opt_bandwidth, min_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 PDF для оптимальной ширины ядра [[toc]](#toc) [[up]](#kernel_pdf_opt_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = KernelDensity(bandwidth=opt_bandwidth)\n",
    "kde.fit(samples[:,None])\n",
    "pdf = np.exp(kde.score_samples(x_values[:,None]))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x_values, pdf, color='b', label='estimated pdf', zorder=2);\n",
    "plt.plot(x_values, true_pdf, color='r', label='true pdf', zorder=2);\n",
    "plt.legend();\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nonparam_regr'></a>\n",
    "# Непараметрическая регрессия [[toc]](#toc) \n",
    "* [1. Пример ядерной регрессии](#ex_kernel_pdf_sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула Надарая-Ватсона\n",
    "\\begin{gather*}\n",
    "a(x;\\xs{n},\\ys{n}) = \\frac{\\Sum_{i = 1}^n y_j K\\lp\\frac{\\rho(x,x_j)}{h}\\rp}{\\Sum_{j=1}^n K\\lp\\frac{\\rho(x,x_j)}{h}\\rp}\n",
    "\\end{gather*}\n",
    "\n",
    "Оценка риска \n",
    "\\begin{gather*}\n",
    "\\hatJ(h) = \\Sum_{i = 1}^n (y_i - a(x_i;\\xs{n\\backslash i},\\ys{n\\backslash i}))^2 = \n",
    "\\Sum_{i = 1}^{n} \\lp y_i - \\frac{\\sum_{j} y_j K_{ij}}{\\sum_{j}K_{ij}}\\rp^{2} \\cdot \\lp 1 - \\frac{K(0)}{\\sum_{j=1}^n K_{ij}} \\rp^{-2}.\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel:\n",
    "    def __init__(self, loc=0, scale=1):\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Возвращает плотность распределение в точке x.\n",
    "        \"\"\"\n",
    "        return # <YOUR CODE>\n",
    "\n",
    "class NadarayaWatsonRegressor:\n",
    "    def __init__(self, kernel, bandwidth):\n",
    "        self.kernel = kernel\n",
    "        self.bandwidth = bandwidth\n",
    "    def fit(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def predict(self, X):\n",
    "        predictions = # <YOUR CODE>\n",
    "        return predictions\n",
    "    \n",
    "def nw_regression_estimated_risk(X, Y, bandwidth, kernel=GaussianKernel()):\n",
    "    h = bandwidth\n",
    "    nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), h)\n",
    "    nw_regressor.fit(X, Y)\n",
    "    Y_pred = nw_regressor.predict(X)\n",
    "    K_X = (X.reshape((-1, 1)) - X.reshape((1, -1))) / h\n",
    "    K = kernel(K_X)\n",
    "    K_sums = np.sum(K, axis=1)\n",
    "    J = # <YOUR CODE>\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(nw_regression_estimated_risk(X=np.zeros(10), Y=np.arange(10),bandwidth=0.1), \n",
    "                  101.85185185185185)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex_kernel_pdf_sin'></a>\n",
    "## 1. Пример ядерной регрессии [[toc]](#toc) [[up]](#tasks_glass)\n",
    "* [1.1 Создание выборки](#ex_kernel_pdf_sin_create)\n",
    "* [1.2 Первичная оценка функции регрессии](#ex_kernel_pdf_sin_first)\n",
    "* [1.3 Оптимальная ширина ядра](#ex_kernel_pdf_sin_opt)\n",
    "* [1.4 Построение доверительной трубки](#ex_kernel_pdf_sin_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex_kernel_pdf_sin_create'></a>\n",
    "### 1.1 Создание выборки [[toc]](#toc) [[up]](#ex_kernel_pdf_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# Generate sample data\n",
    "X = 15 * rng.rand(100)\n",
    "Y = np.sin(X).ravel()\n",
    "Y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n",
    "\n",
    "x_values = np.linspace(X.min(), X.max(), num=1000)\n",
    "y_values = np.sin(x_values)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(x_values, y_values, color='b', label='$y(x)$')\n",
    "plt.scatter(X, Y, color='r', s=16, label=r'$r(x) + \\varepsilon$ (train)')\n",
    "plt.legend(loc='upper right');\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex_kernel_pdf_sin_first'></a>\n",
    "### 1.2 Первичная оценка функции регрессии [[toc]](#toc) [[up]](#ex_kernel_pdf_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.argsort(X)\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 1\n",
    "nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), bandwidth)\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(X)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(X, Y_pred, color='b', label='$\\hat{{r}}(x)$ by NW with $h = {}$'.format(bandwidth), zorder=2)\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)',\n",
    "         zorder=2)\n",
    "plt.legend()\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$');\n",
    "plt.xlim([X.min() - 0.1, X.max() + 0.1])\n",
    "plt.ylim([Y.min() - 5, Y.max() + 5])\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex_kernel_pdf_sin_opt'></a>\n",
    "### 1.3 Оптимальная ширина ядра  [[toc]](#toc) [[up]](#ex_kernel_pdf_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_range = np.linspace(0.07, 1, 94)\n",
    "risk_values = []\n",
    "for h in bandwidth_range:\n",
    "    risk_values.append(nw_regression_estimated_risk(X, Y, h))\n",
    "risk_values = np.array(risk_values)\n",
    "plt.plot(bandwidth_range, risk_values, color='b', zorder=2)\n",
    "plt.xlabel('$h$'); plt.ylabel(r'$\\hat{J}(h)$');\n",
    "plt.title(r'To finding minimum of $\\hat{J}(h)$');\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_bandwidth = bandwidth_range[np.argmin(risk_values)]\n",
    "print(\"Minimal value of J(h) reached at point h = {}, J(h) = {}\".format(opt_bandwidth, np.min(risk_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = opt_bandwidth\n",
    "nw_regressor = NadarayaWatsonRegressor(GaussianKernel(), bandwidth)\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(X)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(X, Y_pred, color='b',label=r'$\\hat{{r}}(x)$ by NW with $h = {:.2}$'.format(bandwidth))\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)')\n",
    "plt.legend()\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$');\n",
    "plt.title(r'$\\hat{r}(x)$ for optimal value of $h$')\n",
    "plt.xlim([X.min() - 0.1, X.max() + 0.1])\n",
    "plt.ylim([Y.min() - 5, Y.max() + 5])\n",
    "plt.grid(which='both', linestyle='--', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ex_kernel_pdf_sin_conf'></a>\n",
    "### 1.4 Построение доверительной трубки [[toc]](#toc) [[up]](#ex_kernel_pdf_sin)\n",
    "\n",
    "\\begin{gather*}\n",
    "\\hat{\\sigma}^2 = \\frac{1}{2 (n - 1)} \\sum\\limits_{i=1}^{n-1} \\left( y_{i+1} - y_{i}  \\right)\n",
    "\\end{gather*}\n",
    "\n",
    "\n",
    "\\begin{gather*}\n",
    "w_i = \\frac{K\\left(\\frac{x - X_i}{h}\\right)}{\\sum\\limits_{j=1}^{n} K\\left(\\frac{x - X_j}{h}\\right)}\n",
    "\\end{gather*}\n",
    "\n",
    "Доверительная трубка:\n",
    "\n",
    "\\begin{gather*}\n",
    "r_{\\pm}(x) = a(x;\\xs{n},\\ys{n}) \\pm z_{\\alpha} \\hat{\\sigma} \\sqrt{\\sum\\limits_{i=1}^{n} w_i^2}\n",
    "\\end{gather*}\n",
    "\n",
    "\n",
    "\\begin{gather*}\n",
    "z_{\\alpha} = \\Phi^{-1}\\left( \\frac{1 + (1 - \\alpha)^{\\frac{w}{b - a}}}{2} \\right)\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "h = opt_bandwidth # Optimal value\n",
    "n = len(Y)\n",
    "a = np.min(X)\n",
    "b = np.max(X)\n",
    "m = (b - a) / h\n",
    "q = stats.norm.ppf((1 + (1 - alpha) ** (1 / m)) / 2)\n",
    "print('q =', q)\n",
    "sigma = np.sqrt(np.sum((Y[1:] - Y[:-1]) ** 2)  / (2 * (n - 1)))\n",
    "print('sigma =', sigma)\n",
    "\n",
    "x_values = np.linspace(a, b, 400)\n",
    "y_values = np.sin(x_values)\n",
    "\n",
    "kernel = GaussianKernel()\n",
    "\n",
    "# по формулам выше посчитать дисперсию\n",
    "K =  # <YOUR CODE>\n",
    "W =  # <YOUR CODE>\n",
    "se =  # <YOUR CODE>\n",
    "\n",
    "nw_regressor = NadarayaWatsonRegressor(kernel, h)\n",
    "nw_regressor.fit(X, Y)\n",
    "Y_pred = nw_regressor.predict(x_values)\n",
    "\n",
    "lower = Y_pred - q * se\n",
    "upper = Y_pred + q * se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(se[:10], \n",
    "                   [0.37413166, 0.36851477, 0.36278137, 0.35689207, 0.35081499,\n",
    "                    0.34452961, 0.33803114, 0.33133502, 0.32448114, 0.31753688])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.fill_between(x_values, lower, upper, color='b', alpha=0.2, zorder=1)\n",
    "plt.plot(x_values, y_values, color='b', linestyle='-', linewidth=2, label=r'$r(x)$')\n",
    "plt.plot(x_values, lower, color='k', linewidth=1, linestyle='--')\n",
    "plt.plot(x_values, upper, color='k', linewidth=1, linestyle='--')\n",
    "plt.plot(x_values, Y_pred, color='r', linestyle='--', label=r'$\\hat{r}(x)$')\n",
    "plt.plot(X, Y, color='g', marker='o', markersize=4, linestyle='none', label=r'$r(x) + \\varepsilon$ (train)')\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$'); plt.title('{:.1f}\\% confidence band'.format(100 * (1 - alpha)))\n",
    "plt.xlim([X.min() - 0.1, X.max() + 0.1])\n",
    "plt.ylim([Y.min() - 3, Y.max() + 3])\n",
    "plt.legend()\n",
    "plt.grid(which='both', alpha=0.5, linestyle='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
