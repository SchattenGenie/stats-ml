{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural gradient\n",
    "\n",
    "#### Краткий синопсис\n",
    "\n",
    "В этой части мы рассмотрим тему, которая находится на границе тем о матрице Фишера и KL-дивергенции. \n",
    "\n",
    "Натуральные градиенты это метод оптимизации в котором шаг оптимизации в градиентном спуске делается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План семинара:\n",
    "\n",
    "Нормальные градиенты:\n",
    "\n",
    "  1. Ввести студентов в курс дела, показать 2D-нормальное распределение;\n",
    "  2. Посчитать log-likelihood и его градиент;\n",
    "  3. Показать, что KL-дивергенция никак не зависит от параметризации;\n",
    "  4. Сделать градиентный спуск в трёх параметризациях и сравнить их(вспомнив азы оптимизации);\n",
    "  5. Реализовать подсчёт матрицы Фишера;\n",
    "  6. Переход к натуральным градиентам: сравнить скорость сходимости(в естественной параметризации должна быть такая же скорость сходимости);\n",
    "  7. Рассказать о том что шаг по натуральному градиенту <=> argmin KL(p|q)\n",
    "  8. Показать что для экспоненциального семейства матрица Фишера это вторая производная по параметрам от нормализационной константы(и сравнить численный подсчёт с этим).\n",
    "  \n",
    "Вассерштайн:\n",
    "  1. Теор справка\n",
    "  2. Реализовать подсчёт Вассерштайна в лоб(полный перебор ~n^2)\n",
    "  3. Реализовать по-умному(сортировка, ~n log n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood оптимизация\n",
    "\n",
    "Очень часто в машинном обучении и статистике возникает задача когда есть данные $X$, некоторое параметрическое распределение $p(x | \\theta)$ и желание найти параметры $\\theta$. \n",
    "\n",
    "Самые лучшие параметры это те которые минимизируют функцию правдоподобия: $\\mathcal{L}(X, \\theta)$, во многих случаях это можно сделать аналитически(почти для всех известных нам именных распределений).\n",
    "\n",
    "В случае если нет возможности посчитать аналитически MLE-оценки приходится обращаться к итеративным методам оптимизации. Один из самых популярных способов это градиентный спуск, который, формально решает следующую задачу:\n",
    "\n",
    "$$d \\theta = \\arg \\max \\mathcal{L}(X|\\theta + d \\theta), s.t. ||d\\theta|| < \\epsilon$$\n",
    "\n",
    "И результатом решения этой задачи является:\n",
    "\n",
    "$$d\\theta = \\frac{\\nabla_\\theta \\mathcal{L}}{||\\nabla_\\theta \\mathcal{L}||} \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимально ли это? \n",
    "\n",
    "Нет. Мы ограничиваем наш шаг $||d\\theta||$ сферой радиусом $\\epsilon$ в евклидовом пространстве, но это неразумно.\n",
    "\n",
    "К примеру, сравним два распределения с $\\mu_1 = -2, \\mu_2 = 2$, а дисперсия фиксирована и равна $\\sigma_1 = \\sigma_2 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import norm\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.plot(x, norm.pdf(x, -2, 1))\n",
    "plt.plot(x, norm.pdf(x, 2, 1))\n",
    "plt.axvline(-2, linestyle='--')\n",
    "plt.axvline(2, linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сравним два распределения с $\\mu_1 = -2, \\mu_2 = 2$, c $\\sigma_1 = \\sigma_2 = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.plot(x, norm.pdf(x, -2, 5))\n",
    "plt.plot(x, norm.pdf(x, 2, 5))\n",
    "plt.axvline(-2, linestyle='--')\n",
    "plt.axvline(2, linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чисто по евклидовой метрики растояние между ними одинаково, но на второй картинке распределения более похожие друг на друга чем на первой.\n",
    "\n",
    "Это можно увидеть если вместо евклидовой метрики смотреть на растояние Кульбака-Лейблера между ними:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(norm.pdf(x, -2, 5), norm.pdf(x, 2, 5)), entropy(norm.pdf(x, -2, 1), norm.pdf(x, 2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этой причине мы бы хотели научиться решать следующую задачу:\n",
    "\n",
    "$$d \\theta = \\arg \\max \\mathcal{L}(X|\\theta + d \\theta), s.t. KL\\left( p(x| \\theta) || p(x| \\theta + d \\theta) \\right) < \\epsilon$$\n",
    "\n",
    "Этим мы и займёмся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных\n",
    "\n",
    "Сгенерируем данные из распределения 2D-гауссианы с 5 параметрами: $\\vec{\\mu} = [2, -6]$, $\\Sigma = \\begin{pmatrix}\n",
    "5 & 0.8\\\\\n",
    "0.8 & 2\n",
    "\\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameters = {\n",
    "    'mean': np.array([2, -6]),\n",
    "    'cov': np.array([[5, 0.8], \n",
    "                     [0.8, 2]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_decart = np.random.multivariate_normal(**true_parameters, size=(2000,))\n",
    "plt.scatter(X_decart[:, 0], X_decart[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ввод функция для расчёта log-likelihood и pdf.\n",
    "\n",
    "Первым делом, вспомним как выглядят плотность и log-likelihood для 2D-гаусса.\n",
    "\n",
    "\n",
    "$$p(x|\\mu, \\Sigma) = \\frac{1}{2 \\pi |\\Sigma|^{1/2}} \\exp\\left( - \\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\Sigma^{-1} (\\vec{x} - \\vec{\\mu}) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_pdf(x: np.ndarray, mean: np.ndarray, cov: np.ndarray):\n",
    "    \"\"\"\n",
    "    Посчитать вероятность пронаблюдать x при фиксированных mean и матрице cov\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and mean.ndim == 1\n",
    "    assert len(x) == 2 and len(mean) == 2\n",
    "    <YOUR CODE>\n",
    "    return ll\n",
    "\n",
    "assert np.abs(norm_2d_pdf(np.array([2., -6.]), **true_parameters) - 0.052021420545533555) < 1e-8\n",
    "assert np.abs(norm_2d_pdf(np.array([1., -4.]), **true_parameters) - 0.013538015908347703) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_ll(x: np.ndarray, mean: np.ndarray, cov: np.ndarray):\n",
    "    \"\"\"\n",
    "    Посчитать логарифм правдоподобия для одного x при фиксированных mean и матрице cov\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and mean.ndim == 1\n",
    "    assert len(x) == 2 and len(mean) == 2\n",
    "    <YOUR CODE>\n",
    "    return ll\n",
    "\n",
    "assert np.abs(norm_2d_ll(np.array([-40., 33.3]), **true_parameters) + 745.0185997116541) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_ll_array(X, mean, cov):\n",
    "    ll = 0\n",
    "    for x in X:\n",
    "        ll += norm_2d_ll(x, mean, cov)\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент 2D-нормального распределения\n",
    "\n",
    "Мы хотим посчитать $\\frac{\\partial \\log p(\\vec{x}|\\vec{\\theta})}{\\partial \\vec{\\theta}} $, где p это наша плотность вероятности, а $\\theta$ - параметры распределения(среднее и матрица ковариации).\n",
    "\n",
    "Подсказки:\n",
    "\n",
    "$$\\frac{\\partial \\log |X|}{\\partial X} = |X|$$\n",
    "\n",
    "$$\\frac{\\partial \\log |X|}{\\partial X} = |X|$$\n",
    "\n",
    "\n",
    "<Здесь можно вызвать студента к доске, чтобы вспомнить азы дифференциирования по вектору>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2d_grad(x: np.ndarray, mean: np.ndarray, cov: np.ndarray):\n",
    "    \"\"\"\n",
    "    Посчитать градиент от логарифма правдоподобия для одного x при фиксированных mean и матрице cov\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1 and mean.ndim == 1\n",
    "    assert len(x) == 2 and len(mean) == 2\n",
    "    <YOUR CODE>\n",
    "    return grad_mean, grad_cov\n",
    "\n",
    "\n",
    "assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчёт матрицы Фишера\n",
    "\n",
    "Флэшбек с прошлой лекции и семинара:\n",
    "\n",
    "$$I_n = E\\left[ \\left( \\frac{\\partial p(\\vec{x}|\\vec{\\theta})}{\\partial \\vec{\\theta}} \\right) \n",
    "\\left( \\frac{\\partial p(\\vec{x}|\\vec{\\theta})}{\\partial \\vec{\\theta}}\\right)^T  \\right]$$\n",
    "\n",
    "Производную мы посчитали выше. А как нам посчитать матожидание?\n",
    "\n",
    "Усреднение по элементам выборки есть приближение среднего,поэтому самый простой способ это посчитать матрицу Фишера для каждого элемента нашей выборки и усреднить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union\n",
    "def empirical_fisher(grad_func: Callable, X: np.ndarray, parameters: dict) -> Union[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция принимает на вход матрицу X размера N x 2, функцию расчёта градиента\n",
    "    для среднего и ковариации и параметры нормального распределения.\n",
    "    \n",
    "    На выход возвращается усреднённый по выборке градиент и матрица Фишнера.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    grads = []\n",
    "    for x in X:\n",
    "        # Посчитать градиент для сэмпла\n",
    "        <YOUR CODE>\n",
    "        # \"Вытянуть\" градиент в один вектор не забыв про условие симметричности матрицы ковариации\n",
    "        <YOUR CODE>\n",
    "        grads.append(grad)\n",
    "    grads = np.vstack(grads)\n",
    "    \n",
    "    # посчитать матрицу фишера по матрице градиентов\n",
    "    fisher = <YOUR CODE>\n",
    "    return grads.mean(axis=0), fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_covariance(vec):\n",
    "    \"\"\"\n",
    "    Просто функция для перевода вектора с len=3 в матрицу 2 x 2.\n",
    "    \"\"\"\n",
    "    m = np.zeros((2, 2))\n",
    "    m.ravel()[[0, 1, 3]] = vec\n",
    "    m[1, 0] = m[0, 1]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL дивергенция\n",
    "\n",
    "Мы будем сравнивать наши распределения по трём метрикам: log-likelihood, MSE по параметрам и KL-дивергенция.\n",
    "\n",
    "$$KL\\left(p(x|\\theta)||p(x|\\theta')\\right) = \\int p(x|\\theta) \\log \\frac{p(x|\\theta)}{p(x|\\theta')} dx $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_difference(parameters_1, parameters_2):\n",
    "    res = (((parameters_1['mean'] - parameters_2['mean'])**2).sum() + \n",
    "          ((parameters_1['cov'] - parameters_2['cov'])**2).sum())\n",
    "    return res / (2 + 4)\n",
    "\n",
    "def kl_difference(parameters_1, parameters_2):\n",
    "    kl = 0\n",
    "    <YOUR CODE>\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Связь KL-дивергенции и матрицы Фишера\n",
    "\n",
    "Ещё раз запишем формулу:\n",
    "\n",
    "$$KL\\left(p(x|\\theta)||p(x|\\theta')\\right) = \\int p(x|\\theta) \\log \\frac{p(x|\\theta)}{p(x|\\theta')} dx $$\n",
    "\n",
    "Теперь возьмём первую производную по $\\theta'$:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\nabla_{\\theta'} KL\\left(p(x|\\theta)||p(x|\\theta')\\right) = E\\left[ \\nabla_{\\theta'} \\log p(x|\\theta')\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "lr = 1e-2\n",
    "sgd_mse = []\n",
    "sgd_mse.append(mse_difference(true_parameters, parameters))\n",
    "\n",
    "sgd_kl = []\n",
    "sgd_kl.append(kl_difference(true_parameters, parameters))\n",
    "\n",
    "sgd_mle = []\n",
    "sgd_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    grad_mean_total = np.zeros(shape=2)\n",
    "    grad_cov_total = np.zeros(shape=(2, 2))\n",
    "    for x in X_decart:\n",
    "        grad_mean, grad_cov = norm_2d_grad(x, **parameters)\n",
    "        grad_mean_total += grad_mean / len(X_decart)\n",
    "        grad_cov_total += grad_cov / len(X_decart)\n",
    "        \n",
    "    parameters['mean'] += lr * grad_mean_total\n",
    "    parameters['cov'] += lr * grad_cov_total\n",
    "    \n",
    "    sgd_mse.append(mse_difference(true_parameters, parameters))\n",
    "    sgd_kl.append(kl_difference(true_parameters, parameters))\n",
    "    sgd_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "    \n",
    "print('mean', parameters['mean'], true_parameters['mean'])\n",
    "print('cov', parameters['cov'], true_parameters['cov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "\n",
    "lr = 1e-2\n",
    "nat_grad_mse = []\n",
    "nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "\n",
    "nat_grad_kl = []\n",
    "nat_grad_kl.append(kl_difference(true_parameters, parameters))\n",
    "\n",
    "nat_grad_mle = []\n",
    "nat_grad_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    grad_mean_total = np.zeros(shape=2)\n",
    "    grad_cov_total = np.zeros(shape=(2, 2))\n",
    "    grad, fisher = empirical_fisher(norm_2d_grad, X=X_decart, parameters=parameters)\n",
    "    natural_grad = <YOUR CODE>\n",
    "    \n",
    "    parameters['mean'] += lr * natural_grad[:2]\n",
    "    parameters['cov'] += lr * vec_to_covariance(natural_grad[2:])\n",
    "    \n",
    "    nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "    nat_grad_kl.append(kl_difference(true_parameters, parameters))\n",
    "    nat_grad_mle.append(norm_2d_ll_array(X_decart, **parameters))\n",
    "\n",
    "    \n",
    "print('mean', parameters['mean'], true_parameters['mean'])\n",
    "print('cov', parameters['cov'], true_parameters['cov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_mse, label='Natural gradient')\n",
    "plt.plot(sgd_mse, label='SGD')\n",
    "plt.title('MSE сравнение')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_kl, label='Natural gradient')\n",
    "plt.plot(sgd_kl, label='SGD')\n",
    "plt.title('KL сравнение')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('KL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_mle, label='Natural gradient')\n",
    "plt.plot(sgd_mle, label='SGD')\n",
    "plt.title('MLE сравнение')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('KL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительное задание\n",
    "\n",
    "Выше мы считали матрицу Фишера эмпирическим путём. Это нормальный способ если выборка большая или если интегрировать по плотности распределения сложно, к примеру, если плотность распределения задаётся нейронной сетью.\n",
    "\n",
    "Для нормального распределения матрицу Фишера не сложно посчитать аналитическим путём. \n",
    "\n",
    "Напишите свой код для аналитического расчёта матрицы Фишера в функции ниже(ничего кроме параметров распределения вам не понадобится).\n",
    "\n",
    "Запустите код и скажите что изменилось?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_fisher(parameters: dict) -> Union[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция принимает на вход матрицу X размера N x 2, функцию расчёта градиента\n",
    "    для среднего и ковариации и параметры нормального распределения.\n",
    "    \n",
    "    На выход возвращается усреднённый по выборке градиент и матрица Фишнера.\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    grads = []\n",
    "    for x in X:\n",
    "        grad_mean, grad_cov = grad_func(x, **parameters)\n",
    "        grad = np.concatenate([grad_mean, grad_cov.ravel()[[0, 1, 3]]])\n",
    "        grads.append(grad)\n",
    "    \n",
    "    \n",
    "    raise ValueError('Where is Fisher, dude?')\n",
    "    \n",
    "    return np.vstack(grads).mean(axis=0), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[1, 0.], \n",
    "                     [0., 1]])\n",
    "}\n",
    "lr = 1e-2\n",
    "nat_grad_mse = []\n",
    "nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "\n",
    "nat_grad_kl = []\n",
    "nat_grad_kl.append(kl_difference(true_parameters, parameters))\n",
    "\n",
    "for i in tqdm(range(200)):\n",
    "    grad_mean_total = np.zeros(shape=2)\n",
    "    grad_cov_total = np.zeros(shape=(2, 2))\n",
    "    grad, fisher = analytical_fisher(norm_2d_grad, X=X_decart, parameters=parameters)\n",
    "    natural_grad = <YOUR CODE>\n",
    "    \n",
    "    parameters['mean'] += lr * natural_grad[:2]\n",
    "    parameters['cov'] += lr * vec_to_covariance(natural_grad[2:])\n",
    "    \n",
    "    nat_grad_mse.append(mse_difference(true_parameters, parameters))\n",
    "    nat_grad_kl.append(kl_difference(parameters, true_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(nat_grad_kl, label='Natural gradient')\n",
    "plt.plot(sgd_kl, label='SGD')\n",
    "plt.title('Comparison')\n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('KL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstain distance\n",
    "\n",
    "Общая формула для расстояния Вассерштейна:\n",
    "\n",
    "$$W_p(p, q) = \\inf \\int\\limits_{\\mathbb{R} \\times \\mathbb{R}} d(x, y)^p d \\pi(x, y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "from itertools import permutations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_brute_force(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    min_distance = np.inf\n",
    "    argmin_permutation = None\n",
    "    for permutation in permutations(np.arange(len(x))):\n",
    "        <YOUR CODE>\n",
    "    return min_distance\n",
    "\n",
    "\n",
    "x = np.random.randn(4)\n",
    "y = np.random.randn(4)\n",
    "assert np.abs(wasserstein_distance_brute_force(x, y) - wasserstein_distance(x, y)) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_distance_smart_force(x, y):\n",
    "    x_idx = np.argsort(x)\n",
    "    y_idx = np.argsort(y)\n",
    "    distance = <YOUR CODE>\n",
    "    return distance\n",
    "\n",
    "wasserstein_distance_brute_force(x, y) == wasserstein_distance_smart_force(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "\n",
    "x = np.random.randn(N) - 3\n",
    "y = np.random.randn(N) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, push_notebook, show\n",
    "from bokeh.models import CustomJS, Slider\n",
    "from bokeh.layouts import row\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "bw = 0.01\n",
    "\n",
    "x_linspace = np.linspace(-10, 10, 1000)\n",
    "density_x = gaussian_kde(x, bw_method=bw)\n",
    "density_y = gaussian_kde(y, bw_method=bw)\n",
    "fig = figure()\n",
    "density_x_bokeh = fig.line(x_linspace, density_x(x_linspace), color=\"red\")\n",
    "density_y_bokeh = fig.line(x_linspace, density_y(x_linspace), color=\"blue\")\n",
    "\n",
    "\n",
    "\n",
    "def update_plot(n_moved):\n",
    "    x_linspace = density_x_bokeh.data_source.data['x']\n",
    "    \n",
    "    x_idx = np.argsort(x)\n",
    "    y_idx = np.argsort(y)\n",
    "    \n",
    "    x_copy = x.copy()\n",
    "    print(n_moved)\n",
    "    for i in range(n_moved):\n",
    "        x_copy[x_idx[i]] = y[y_idx[i]]\n",
    "    \n",
    "    density_x = gaussian_kde(x_copy, bw_method=bw)\n",
    "    density_x_bokeh.data_source.data['y'] = density_x(x_linspace)\n",
    "    push_notebook(handle=bokeh_handle)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = CustomJS(code=\"\"\"\n",
    "if (IPython.notebook.kernel !== undefined) {\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    cmd = \"update_plot(\" + cb_obj.value + \")\";\n",
    "    kernel.execute(cmd, {}, {});\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "slider = Slider(start=0, \n",
    "                end=len(x),\n",
    "                value=0,\n",
    "                step=1,\n",
    "                title=\"Moved dots\",\n",
    "                callback=callback)\n",
    "bokeh_handle = show(row(fig, slider), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
