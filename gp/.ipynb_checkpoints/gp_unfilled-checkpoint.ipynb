{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes\n",
    "\n",
    "## План\n",
    "\n",
    "  * Напишем свой простейший GP;\n",
    "  * Посмотрим на библиотеку `GPy`;\n",
    "  * С помощью `GPytorch` научимся использовать Scalable GP;\n",
    "  * Научимся объединять Deep Learning с GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "EPS = 1e-10\n",
    "sns.set()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def plot_gp(X_train, y_train, X_test, samples, mu, std):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "\n",
    "    plt.plot(X_test, samples.T)\n",
    "\n",
    "    plt.fill_between(X_test.ravel(), \n",
    "                     mu.ravel() - 2 * std.ravel(), \n",
    "                     mu.ravel() + 2 * std.ravel(), alpha=0.3)\n",
    "\n",
    "    plt.plot(X_test, mu, 'r--', lw=2, label='Mean of GP')\n",
    "    \n",
    "    plt.scatter(X_train, y_train, s=30, label='Original data')\n",
    "\n",
    "    plt.title('GP')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Распределения: совместные, условные, частные\n",
    "\n",
    "Совместное распределение:\n",
    "\n",
    "$$(x, y) \\sim \\mathcal{N}\\left(\\mu, \\Sigma\\right), ~\\mu = [\\mu_1, \\mu_2], ~~\\Sigma=\\begin{bmatrix}\\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{11}^T & \\Sigma_{22} \\end{bmatrix}$$\n",
    "\n",
    "Частное:\n",
    "\n",
    "$$x \\sim  \\mathcal{N}(\\mu_1, \\Sigma_{11})$$\n",
    "\n",
    "\n",
    "Условное:\n",
    "\n",
    "$$ (x | y) \\sim \\mathcal{N}(\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y - \\mu_2), \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{12}^T)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'mean': np.array([0., 0.]),\n",
    "    'cov': np.array([[2., 1.5], \n",
    "                     [1.5, 2.]])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Совместное и частные распределения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(size=5000, **parameters)\n",
    "sns.jointplot(x=X[:, 0], y=X[:, 1], kind=\"hex\", color=\"k\", size=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Условные распределения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.linspace(-5, 5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = parameters['mean'][0] + parameters['cov'][0, 1] * (y - parameters['mean'][1]) / parameters['cov'][1, 1]\n",
    "\n",
    "var = parameters['cov'][0, 0] - parameters['cov'][0, 1]**2 / parameters['cov'][1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y, mu)\n",
    "plt.fill_between(y, mu - 2 * np.sqrt(var), mu + 2 * np.sqrt(var), alpha=0.2)\n",
    "plt.title('Условное распределение x|y')\n",
    "plt.ylabel('Mean of x')\n",
    "plt.xlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ядра и, в частности, гауссово ядро\n",
    "\n",
    "$$\\mathrm{kern}: X \\times X \\rightarrow \\mathbb{R} $$\n",
    "\n",
    "Ядро это функция, которая отображает декартово произведение некоторого пространства с самим собой на действительную ось.\n",
    "\n",
    "Больше инфы про ядра: http://www.machinelearning.ru/wiki/index.php?title=%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D1%8F%D0%B4%D1%80%D0%B0\n",
    "\n",
    "#### Гауссово ядро\n",
    "\n",
    "$$\\mathrm{kern}(x, y) \\sim \\exp\\left( -\\frac{||x - y||_2}{2 \\sigma^2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def __call__(self, x_train, x_test):\n",
    "        # l2-норма между матрицами x_train[N, K] y_train[M, K]\n",
    "        # Выходом должна быть матрица [N, M]\n",
    "        dist =  # <YOUR CODE>\n",
    "        k = np.exp(- dist / self.sigma**2 / 2)\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GaussianKernel(sigma=1.)\n",
    "\n",
    "assert np.allclose(\n",
    "    kern(np.linspace(0, 1, 3).reshape(-1, 1), np.linspace(1, 2, 4).reshape(-1, 1)),\n",
    "    [[0.60653066, 0.41111229, 0.24935221, 0.13533528],\n",
    "     [0.8824969 , 0.70664828, 0.50633562, 0.32465247],\n",
    "     [1.        , 0.94595947, 0.8007374 , 0.60653066]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Причём здесь гауссовы процессы?\n",
    "\n",
    "Пусть у нас есть выборка $X_{train}, y_{train}$ и есть $X_{test}$ для которых мы хотим построить вероятностную модель для $y_{test}$. А ещё мы выбрали некоторое ядро $\\mathrm{kern}(\\cdot, \\cdot)$.\n",
    "\n",
    "Тогда гауссова регрессия(гауссов процесс) записывается следующим образом:\n",
    "\n",
    "$$p(X_{test}, y_{test}, X_{train}, y_{train}) = \\mathcal{N}\\left( 0 , \\begin{bmatrix}K + \\sigma^2 I & K_* \\\\ K_*^T & K_{**} \\end{bmatrix} \\right), $$\n",
    "\n",
    "где\n",
    "\n",
    "$$K = \\mathrm{kern}(X_{train}, X_{train})$$\n",
    "\n",
    "$$K_{*} = \\mathrm{kern}(X_{train}, X_{test})$$\n",
    "\n",
    "$$K_{**} = \\mathrm{kern}(X_{test}, X_{test})$$\n",
    "\n",
    "Т.е. матрица ковариаций это матрица ядра. Как мы увидим дальше, выбор ядра __очень важен__.\n",
    "\n",
    "Тогда наша задача это найти следующее распределение:\n",
    "\n",
    "$$p(y_{test} | X_{test}, X_{train}, y_{train}) \\sim \\mathcal{N}\\left(K_{*}^T (K + \\sigma^2 I)^{-1} y_{train}, K_{**} - K_{*} (K + \\sigma^2 I)^{-1} K_{*}^T\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noiseless training data\n",
    "X_train = np.linspace(-5, 1, 40).reshape(-1, 1)\n",
    "y_train = np.sin(2 * X_train) + np.random.randn(*X_train.shape) / 2 + X_train / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "X_test = np.linspace(-6, 12, N).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.ravel(), y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объяснение про разложение Холецкого и зачем оно здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianRegression:\n",
    "    def __init__(self, kernel, X, y, noise=EPS):\n",
    "        self.kernel = kernel\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.noise = noise\n",
    "        \n",
    "        self.K_train = self.kernel(self.X_train, self.X_train)\n",
    "        self.L_train = np.linalg.cholesky(self.K_train + self.noise * np.eye(len(X_train)))\n",
    "        self.K_inv = np.linalg.pinv(self.K_train + self.noise * np.eye(len(X_train)))\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # train-test kernel matrix\n",
    "        k_train_test =  # <YOUR CODE>\n",
    "        \n",
    "        # find mean(formules above)\n",
    "        mu = # <YOUR CODE>\n",
    "        \n",
    "        # find test-test kernel matrix\n",
    "        K_test_test =  # <YOUR CODE>\n",
    "        \n",
    "        # cov matrix\n",
    "        cov = # <YOUR CODE>\n",
    "        \n",
    "        # std\n",
    "        std = np.sqrt(np.diag(cov))\n",
    "        \n",
    "        return mu.reshape((len(X_test), -1)), std.reshape((len(X_test), -1))\n",
    "    \n",
    "    def sample(self, X_test, n=1):\n",
    "        # copy-paste code above\n",
    "        \n",
    "        samples = np.random.multivariate_normal(size=n, mean=mu.ravel(), cov=cov)\n",
    "        \n",
    "        return samples.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GaussianKernel(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gregressor = GaussianRegression(kernel=kernel, X=X_train, y=y_train, noise=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu, std = gregressor.predict(X_test)\n",
    "samples = gregressor.sample(X_test, n=3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp(X_train=X_train, y_train=y_train, \n",
    "        X_test=X_test, samples=samples, mu=mu, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPy\n",
    "\n",
    "Чтобы не прогать самим и не делать кучу ошибок, лучше использовать готовые решения :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = GPy.kern.RBF(input_dim=1, lengthscale=1., variance=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормальные ребята используют для регрессии GPRegression\n",
    "# clf=GPy.models.GPRegression(X_train, y_train, kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# но нам и так нормально\n",
    "clf=GPy.core.GP(X=X_train, \n",
    "                Y=y_train, \n",
    "                kernel=kern, \n",
    "                likelihood=GPy.likelihoods.Gaussian(variance=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.optimize(messages=True, optimizer='scg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, cov = clf.predict(X_test, full_cov=True, include_likelihood=False)\n",
    "std = np.sqrt(np.diag(cov))\n",
    "\n",
    "samples = np.random.multivariate_normal(size=3, mean=mu.ravel(), cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp(X_train=X_train, y_train=y_train, \n",
    "        X_test=X_test, samples=samples, mu=mu, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOAR KERNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern = # <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GPy.core.GP(X=X_train, \n",
    "                Y=y_train, \n",
    "                kernel=kern, \n",
    "                likelihood=GPy.likelihoods.Gaussian(variance=1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.optimize(messages=True, optimizer='scg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, cov = clf.predict(X_test, full_cov=True, include_likelihood=False)\n",
    "std = np.sqrt(np.diag(cov))\n",
    "\n",
    "samples = np.random.multivariate_normal(size=10, mean=mu.ravel(), cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gp(X_train=X_train, y_train=y_train, \n",
    "        X_test=X_test, samples=samples, mu=mu, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас Анаконда, то вам нужно запустить следующую команду для установки PyTorch v1.0:\n",
    "\n",
    "```\n",
    "conda install pytorch-nightly-cpu -c pytorch\n",
    "```\n",
    "\n",
    "Если чистый Питон с pip:\n",
    "```\n",
    "pip install numpy torchvision_nightly\n",
    "pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
    "```\n",
    "\n",
    "\n",
    "После установки PyTorch, устанавливаем gpytorch:\n",
    "\n",
    "```\n",
    "pip install gpytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пару примеров на использование KISS-GP и KISS-GP с Kernel Learning. \n",
    "\n",
    "Примеры почерпнуты от авторов библиотеки: https://github.com/cornellius-gp/gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, n), np.linspace(0, 1, n))\n",
    "train_x = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, dtype=torch.float32)\n",
    "\n",
    "# sin( 2 * pi * (x0+x1))\n",
    "train_y = torch.sin((train_x[:, 0] + train_x[:, 1]) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=train_y);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = # <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel, num_dims=2,):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(kernel,), \n",
    "            grid_size=grid_size, num_dims=num_dims\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood, kernel=kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор нейронки\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лосс-функция\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(training_iterations = 30):\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "xx, yy = np.meshgrid(np.linspace(-1, 2, n), np.linspace(-1, 2, n))\n",
    "test_x = torch.tensor(np.vstack([xx.ravel(), yy.ravel()]).T, dtype=torch.float32)\n",
    "test_y_actual = torch.sin(((test_x[:, 0] + test_x[:, 1]) * (2 * math.pi))).view(n, n)\n",
    "\n",
    "with torch.no_grad(), gpytorch.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    pred_labels = observed_pred.mean.view(n, n)\n",
    "\n",
    "delta_y = torch.abs(pred_labels - test_y_actual).detach().numpy()\n",
    "\n",
    "def ax_plot(f, ax, y_labels, title):\n",
    "    im = ax.imshow(y_labels)\n",
    "    ax.set_title(title)\n",
    "    f.colorbar(im)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 8))\n",
    "    \n",
    "# Предсказания\n",
    "ax[0].imshow(pred_labels)\n",
    "ax[0].set_title('Predicted Values (Likelihood)')\n",
    "\n",
    "# Ground truth\n",
    "ax[1].imshow(test_y_actual)\n",
    "ax[1].set_title('Actual Values (Likelihood)')\n",
    "\n",
    "# Ошибки\n",
    "ax[2].imshow(delta_y)\n",
    "ax[2].set_title('Absolute Error Surface')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalabel GP + Deep Learning\n",
    "\n",
    "Зачем подбирать ядра и мучиться если можно обучить нейросетку?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "if not os.path.isfile('elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):           \n",
    "    def __init__(self):                                      \n",
    "        super(LargeFeatureExtractor, self).__init__()        \n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())                  \n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))     \n",
    "        self.add_module('relu2', torch.nn.ReLU())                  \n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))         \n",
    "                                                             \n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x):\n",
    "            # пропускаем данные через нейронку\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "        \n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "likelihood.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_iterations = 60):\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "        \n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
